{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Final Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert title here\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Angel Olivas\n",
    "\n",
    "- Hyunseo Park\n",
    "\n",
    "- Yuanzhen Zhu\n",
    "\n",
    "- Eric Dong"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This paper aims to research and understand the possibility of looking at how one can use machine learning models to analyze and predict whether or not a credit card will default. Defaulting on credit card payments occurs when individuals repeatedly fail to pay their bills, leading to account closure and negative marks on their credit history. The consequences of defaulting are severe and can create problems for both lenders and borrowers. Thus, the goal is to figure out when a credit card bill would default to either prevent or notify both parties from dealing with potential consequences. The study utilizes a dataset from Taiwan, encompassing various client statistics such as credit accumulation, payment history, billing amounts, and more. By analyzing the data of around 30,000 clients, the research investigates the relationship between these factors and credit cards defaulting. Six different data mining methods were employed to extract meaningful insights and patterns from the dataset, forming the basis for us to clean and manipulate the data in a way that allows us to build a machine learning model to predict debt default in a binary way. For our machine learning model, we decided on binary logistic regression as the modeling technique due to its ability to handle binary classification tasks effectively. It allows for the estimation of the probability of credit cards defaulting based on the identified factors. To evaluate the performance of the model, we will employ AUC-ROC curves, with a specific focus on minimizing the false positive rate. This emphasis on accuracy is essential, as providing incorrect information regarding credit card default risk can have significant repercussions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Understanding and preventing defaulting on credit card loans is an issue that is very vital in dealing with financial risk management. In allowing the credit card account to be closed due to unpaid charges, credit histories are ruined and financial burdens and stresses are exponentially increased. There are required debts to be paid and any mortgage, student, or auto loans will also take a hit or be increased. The burden on the loaner increases, and they might resort to harsher tactics as time passes.<a name=\"Akinnote\"></a>[<sup>[1]</sup>](#Akin) <br><br>\n",
    "In economic models, certain relationships between variables, known as \"stylized facts,\" can provide valuable insights into the behavior of credit card default. The process of creating such models often rely on broad trends that are quantified into aggregate satistics into how resources are distributed and flow in a society. However there are times where fundamental relationships between variables tend to be skewed one way or the other based on external factors leading to some confusion and investigation, these \"stylized facts\" that are essentially just understood relationships that inform us as to what is happening in a system. An example of this would be the relationship between interest rates and investments, which would be that as interest rates goes down, generally speaking investments go up.<a name=\"Ouliarisnote\"></a>[<sup>[2]</sup>](#Ouliaris) One of the most famous example of this being prior to the 2008 financial crisis in United States, as interest rates decreased more people got into real estate that didn't have the capital to be doing so. This eventually lead to a bust because irresponsible lending to these creditors became rampant. But it also isn't unheard of for investments to decrease as interest rates go down, for example in Japan in the 1990's there was a period of uncertainty in the economy that created economic behaviour that needed to be stopped.<a name=\"Neilsennote\"></a>[<sup>[3]</sup>](#Neilsen,) When this is the case, it is important to look at smaller scale models to figure diagonse what is causing odd behaviour, and though it would be ideal to make classifier models that properly reflect the real world, often times predictive models are needed to gain insight to build classifier models. We aim to help create one of these smaller scale models for Taiwan as it's within our ability with the dataset we have, using data found in debt collection and payment history and monthly income level. The choice of our features is partially informed by both historical and newer models for consumer default risk,<a name=\"CostaeSilvanote\"></a>[<sup>[4]</sup>](#CostaeSilva,) and aims to create a smaller model for analysis for tracking debt in Taiwan. Ultimately, our goal is to enhance risk assessment practices, facilitate early detection of defaulting behaviors, and foster financial stability in the credit card industry. <br><br>\n",
    "Previous work in this field has been done, such as various studies done by in Malaysia<a name=\"Sayjadahnote\"></a>[<sup>[5]</sup>](#Sayjadah,), India<a name=\"Mahmudinote\"></a>[<sup>[6]</sup>](#Mahmudi,), and UCLA<a name=\"Guinote\"></a>[<sup>[7]</sup>](#Gui), all looking at using Machine Learning to predict credit card defaults. All three studies were conclusive, using varying machine learning techniques and finding which ones concluded with high precision rates, including random forest, XGBoost, and AdaBoost. These replicable studies show the application of machine learning models, highlighting their effectiveness in alleviating financial management burdens. By utilizing advanced techniques and analyzing relevant datasets, researchers have successfully developed models that provide conclusive insights into credit card default prediction. The findings from these studies serve as valuable references for our research, indicating that machine learning can yield robust models and contribute to improved financial decision-making. By leveraging the knowledge gained from previous studies, we aim to further investigate and build upon these findings. Our objective is to build successful models, contributing to research and providing insight for banks and clients dealing with credit card defaults."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The problem that we are aiming to solve is whether or not a certain client's credit card would default given different variables and factors. Some very useful information that we could use to create a model to predict a client's possibility to default - which is very clearly just a Yes or No. Some potential solutions that we were looking at given that the result is a binary classification of either positive or negative are either looking at Logistic Regression models or maybe even a Suport Vector Machine. Given our dataset, we have around 30 thousand clients to work with and approximately 23 explanatory variables that are chunked into around eight parts. However, the result of the problem should be entirely binary, a positive or negative stat as to whether or not the credit card account would default. All the explantory variables are measurable as the data is essentially just data about the clients and their credit history. In addition, the entire problem would be replicable as we can continue looking at different datasets and training our model based off of those, predicting whether or not certain credit card accounts would default."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "- Link to our dataset https://www.kaggle.com/datasets/utkarshx27/default-of-credit-card-clients-dataset\n",
    "\n",
    "- Dataset name: 'Predict Credit Card Defaulters'\n",
    "\n",
    "- The original dataset contains 25 columns/variables, and 30001 total observations\n",
    "\n",
    "- An observation consists of 24 columns of X variables that in different degrees might relates to whether someone will default, and one y variable default payment (Yes = 1, No = 0) which is the classification of the observation\n",
    "\n",
    "- Some critical variables potentially include Amount of credit given, Education, Marital status, Age, History of past payment. Amount of credit given and Age are represented in continuous natural numbers, Education and Marital status are represented categorically by natural numbers, History of past payment is represented monthly across multiple columns with each column denoting number of months that the payment has been late with discrete numbers (-1 ~ 9)\n",
    "\n",
    "- Cleaning: This dataset don't have empty/null entries. There are 0 values in Education, Marial status variables however, We decided to combine those entries into others categories. We trimmed the first row of data to match variables.\n",
    "  Transformation: We Z-scored Past payment, and then transformed Amount of bill statement & Amount of previous payment by taking the Mean of (Amount of bill statement - Amount of previous payment) and Z-scored it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "Our solution aims to make features of data found in the columns for credit given, repayment status, amount of bills, education, and amount paid per month. As mentioned in data, some of our feature selection involves aggregate statistics that normalized through using z-score and normal curve. We are hopeful about this method of normalization, however we recognize that z-score is particularly sensitive toward skews in existing data. The reason we have to normalize in the first place is because some of our variables are within the magnitude of single digits, while some debt is within four or five. However we acknowledge this dataset may not follow the normal distribution well enough to use it, if this is the case, then we plan to explore L1 normalization techinques to correct for this issue as this method resists outliers by nature. As for the type of statistical model, we will be using logistic regression in order to assign a probability of the predicted outcome of a creditor defaulting or paying their debts off, with this in mind we would be able to more easily interpret the relationships between different features. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "One evaluation metrics we could use is AUC-ROC, which is 'Area under the ROC Curve'. This is a widely recognized evaluation metrics for classification model. This is derived by graphing the model's 'True positive rate'(True positive/(True Positive + False Negative)) versus 'False positive rate'(1 - (True Negative/(True Negative + False Positive))) which is always in conflicting tradeoffs in most models. A better model is characterized by having a bigger area under the ROC curve which says the model would have a larger TP rate and a smaller FP rate in its optimum point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Subsection 1\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "### Subsection 2\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Maybe you include a learning curve to show whether you have enough data to do train/validate/test split or have to go to k-folds or LOOCV or ???\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "We recognize that our project may have real data privacy and ethics issues depending on what variables we decide to put into creating our model. Some variables such as Gender, Marriage status and Age may be very sensitive and private information, we will make sure that datas we use are completely anonymous and will not specify or harm any real entities that the data may subject to. Additionally, we will make sure that our model will be for research purposes only and will not be used to discriminate against any group or any person. Lastly, we can utilize tools such as 'deon' to check for data ethics issue.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"Akinnote\"></a>1.[^](#Akin): Akin, J. (1 Nov 2022) How Does Default Impact Your Credit?. *Experian*. https://www.experian.com/blogs/ask-experian/how-does-default-impact-credit/ <br>\n",
    "<a name=\"Ouliarisnote\"></a>2.[^](#Ouliaris): Ouliaris, S. (9 Dec 2021) What are Economic Models?. *International Monetary Fund*. https://www.imf.org/external/pubs/ft/fandd/2011/06/basics.html <br>\n",
    "<a name=\"Neilsennote\"></a>3.[^](#Neilsen,): Neilsen, B. (14 Jan 2022) The Lost Decade: Lessons from Japan's Real Estate Crisis. *Investopedia*. https://www.investopedia.com/articles/economics/08/japan-1990s-credit-crunch-liquidity-trap.asp <br>\n",
    "<a name=\"CostaeSilvanote\"></a>4.[^](#CostaeSilva,): Costa e Silva, E. (5 May 2020) TA logistic regression model for consumer default risk. *National Library of Medicine*. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9041570/ <br>\n",
    "<a name=\"Sayjadahnote\"></a>5.[^](#Sayjadah,): Sayjadah, Y. et al. (28 Oct 2018) Credit Card Default Prediction using Machine Learning Techniques. *IEEE Explore*. https://ieeexplore.ieee.org/document/8776802 <br>\n",
    "<a name=\"Mahmudinote\"></a>6.[^](#Mahmudi,): Mahmudi, H. et al. (15 Oct 2022) Evaluation of Gradient Boosting Algorithms on Balanced Home Credit Default Risk. *IEEE Explore*. https://ieeexplore.ieee.org/document/10041584 <br>\n",
    "<a name=\"Guinote\"></a>7.[^](#Gui): Gui, L. (2019) Application of Machine Learning Algorithms in Predicting Credit Card Default Payment. *UCLA*. https://escholarship.org/uc/item/9zg7157q#main <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
