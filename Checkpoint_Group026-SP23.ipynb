{
	"cells": [
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# COGS 118A - Project Checkpoint"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Names\n",
				"\n",
				"- Angel Olivas\n",
				"\n",
				"- Hyunseo Park\n",
				"\n",
				"- Yuanzhen Zhu\n",
				"\n",
				"- Eric Dong"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Abstract \n",
				"This paper aims to research and understand the possibility of looking at how one can use machine learning models to analyze and predict whether or not a credit card will default. Defaulting on credit card payments occurs when individuals repeatedly fail to pay their bills, leading to account closure and negative marks on their credit history. The consequences of defaulting are severe and can create problems for both lenders and borrowers. Thus, the goal is to figure out when a credit card bill would default to either prevent or notify both parties from dealing with potential consequences. The study utilizes a dataset from Taiwan, encompassing various client statistics such as credit accumulation, payment history, billing amounts, and more. By analyzing the data of around 30,000 clients, the research investigates the relationship between these factors and credit cards defaulting. Six different data mining methods were employed to extract meaningful insights and patterns from the dataset, forming the basis for us to clean and manipulate the data in a way that allows us to build a machine learning model to predict debt default in a binary way. For our machine learning model, we decided on binary logistic regression as the modeling technique due to its ability to handle binary classification tasks effectively. It allows for the estimation of the probability of credit cards defaulting based on the identified factors. To evaluate the performance of the model, we will employ AUC-ROC curves, with a specific focus on minimizing the false positive rate. This emphasis on accuracy is essential, as providing incorrect information regarding credit card default risk can have significant repercussions."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Background\n",
				"\n",
				"Understanding and preventing defaulting on credit card loans is an issue that is very vital in dealing with financial risk management. In allowing the credit card account to be closed due to unpaid charges, credit histories are ruined and financial burdens and stresses are exponentially increased. There are required debts to be paid and any mortgage, student, or auto loans will also take a hit or be increased. The burden on the loaner increases, and they might resort to harsher tactics as time passes.<a name=\"Akinnote\"></a>[<sup>[1]</sup>](#Akin) <br><br>\n",
				"In economic models, certain relationships between variables, known as \"stylized facts,\" can provide valuable insights into the behavior of credit card default. The process of creating such models often rely on broad trends that are quantified into aggregate satistics into how resources are distributed and flow in a society. However there are times where fundamental relationships between variables tend to be skewed one way or the other based on external factors leading to some confusion and investigation, these \"stylized facts\" that are essentially just understood relationships that inform us as to what is happening in a system. An example of this would be the relationship between interest rates and investments, which would be that as interest rates goes down, generally speaking investments go up.<a name=\"Ouliarisnote\"></a>[<sup>[2]</sup>](#Ouliaris) One of the most famous example of this being prior to the 2008 financial crisis in United States, as interest rates decreased more people got into real estate that didn't have the capital to be doing so. This eventually lead to a bust because irresponsible lending to these creditors became rampant. But it also isn't unheard of for investments to decrease as interest rates go down, for example in Japan in the 1990's there was a period of uncertainty in the economy that created economic behaviour that needed to be stopped.<a name=\"Neilsennote\"></a>[<sup>[3]</sup>](#Neilsen,) When this is the case, it is important to look at smaller scale models to figure diagonse what is causing odd behaviour, and though it would be ideal to make classifier models that properly reflect the real world, often times predictive models are needed to gain insight to build classifier models. We aim to help create one of these smaller scale models for Taiwan as it's within our ability with the dataset we have, using data found in debt collection and payment history and monthly income level. The choice of our features is partially informed by both historical and newer models for consumer default risk,<a name=\"CostaeSilvanote\"></a>[<sup>[4]</sup>](#CostaeSilva,) and aims to create a smaller model for analysis for tracking debt in Taiwan. Ultimately, our goal is to enhance risk assessment practices, facilitate early detection of defaulting behaviors, and foster financial stability in the credit card industry. <br><br>\n",
				"Previous work in this field has been done, such as various studies done by in Malaysia<a name=\"Sayjadahnote\"></a>[<sup>[5]</sup>](#Sayjadah,), India<a name=\"Mahmudinote\"></a>[<sup>[6]</sup>](#Mahmudi,), and UCLA<a name=\"Guinote\"></a>[<sup>[7]</sup>](#Gui), all looking at using Machine Learning to predict credit card defaults. All three studies were conclusive, using varying machine learning techniques and finding which ones concluded with high precision rates, including random forest, XGBoost, and AdaBoost. These replicable studies show the application of machine learning models, highlighting their effectiveness in alleviating financial management burdens. By utilizing advanced techniques and analyzing relevant datasets, researchers have successfully developed models that provide conclusive insights into credit card default prediction. The findings from these studies serve as valuable references for our research, indicating that machine learning can yield robust models and contribute to improved financial decision-making. By leveraging the knowledge gained from previous studies, we aim to further investigate and build upon these findings. Our objective is to build successful models, contributing to research and providing insight for banks and clients dealing with credit card defaults."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Problem Statement\n",
				"\n",
				"The problem that we are aiming to solve is whether or not a certain client's credit card would default given different variables and factors. Some very useful information that we could use to create a model to predict a client's possibility to default - which is very clearly just a Yes or No. Some potential solutions that we were looking at given that the result is a binary classification of either positive or negative are either looking at Logistic Regression models or maybe even a Suport Vector Machine. Given our dataset, we have around 30 thousand clients to work with and approximately 23 explanatory variables that are chunked into around eight parts. However, the result of the problem should be entirely binary, a positive or negative stat as to whether or not the credit card account would default. All the explantory variables are measurable as the data is essentially just data about the clients and their credit history. In addition, the entire problem would be replicable as we can continue looking at different datasets and training our model based off of those, predicting whether or not certain credit card accounts would default."
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Data\n",
				"\n",
				"UPDATED FROM PROPOSAL!\n",
				"\n",
				"You should have obtained and cleaned (if necessary) data you will use for this project.\n",
				"\n",
				"Please give the following infomration for each dataset you are using\n",
				"- link/reference to obtain it\n",
				"- description of the size of the dataset (# of variables, # of observations)\n",
				"- what an observation consists of\n",
				"- what some critical variables are, how they are represented\n",
				"- any special handling, transformations, cleaning, etc you have done should be demonstrated here!\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Proposed Solution\n",
				"\n",
				"Our solution aims to make features of data found in the columns for credit given, repayment status, amount of bills, education, and amount paid per month. As mentioned in data, some of our feature selection involves aggregate statistics that normalized through using z-score and normal curve. We are hopeful about this method of normalization, however we recognize that z-score is particularly sensitive toward skews in existing data. The reason we have to normalize in the first place is because some of our variables are within the magnitude of single digits, while some debt is within four or five. However we acknowledge this dataset may not follow the normal distribution well enough to use it, if this is the case, then we plan to explore L1 normalization techinques to correct for this issue as this method resists outliers by nature. As for the type of statistical model, we will be using logistic regression in order to assign a probability of the predicted outcome of a creditor defaulting or paying their debts off, with this in mind we would be able to more easily interpret the relationships between different features. "
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Evaluation Metrics\n",
				"\n",
				"One evaluation metrics we could use is AUC-ROC, which is 'Area under the ROC Curve'. This is a widely recognized evaluation metrics for classification model. This is derived by graphing the model's 'True positive rate'(True positive/(True Positive + False Negative)) versus 'False positive rate'(1 - (True Negative/(True Negative + False Positive))) which is always in conflicting tradeoffs in most models. A better model is characterized by having a bigger area under the ROC curve which says the model would have a larger TP rate and a smaller FP rate in its optimum point."
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Preliminary results\n",
				"\n",
				"NEW SECTION!\n",
				"\n",
				"Please show any preliminary results you have managed to obtain.\n",
				"\n",
				"Examples would include:\n",
				"- Analyzing the suitability of a dataset or alogrithm for prediction/solving your problem \n",
				"- Performing feature selection or hand-designing features from the raw data. Describe the features available/created and/or show the code for selection/creation\n",
				"- Showing the performance of a base model/hyper-parameter setting.  Solve the task with one \"default\" algorithm and characterize the performance level of that base model.\n",
				"- Learning curves or validation curves for a particular model\n",
				"- Tables/graphs showing the performance of different models/hyper-parameters\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### 1. Load data from csv file"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"#Importing dataframes\n",
				"import pandas as pd\n",
				"\n",
				"file = 'clients_dataset.csv'\n",
				"df = pd.read_csv(file)\n",
				"# print(df.head())"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### 2. Cleaning up invalid entries for categorical columns"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Unique values in EDUCATION column: [2 1 3 5 4 6 0]\n",
						"Unique values in MARRIAGE column: [1 2 3 0]\n",
						"\n",
						"Results:\n",
						"Unique values in EDUCATION column: [2 1 3 4]\n",
						"Unique values in MARRIAGE column: [1 2 3]\n"
					]
				}
			],
			"source": [
				"print(\"Unique values in EDUCATION column:\", df[\"EDUCATION\"].unique())\n",
				"print(\"Unique values in MARRIAGE column:\", df[\"MARRIAGE\"].unique())\n",
				"\n",
				"valid_education_val = [1, 2, 3, 4]\n",
				"df[\"EDUCATION\"] = df[\"EDUCATION\"].replace([value for value in df[\"EDUCATION\"] if value not in valid_education_val], 4)\n",
				"\n",
				"valid_marriage_val = [1, 2, 3]\n",
				"df[\"MARRIAGE\"] = df[\"MARRIAGE\"].replace([value for value in df[\"MARRIAGE\"] if value not in valid_marriage_val], 3)\n",
				"\n",
				"print(\"\\nResults:\")\n",
				"print(\"Unique values in EDUCATION column:\", df[\"EDUCATION\"].unique())\n",
				"print(\"Unique values in MARRIAGE column:\", df[\"MARRIAGE\"].unique())"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### 3. One-hot-encode categorical data"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 3,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"   ID  LIMIT_BAL  AGE  PAY_0  PAY_2  PAY_3  PAY_4  PAY_5  PAY_6  BILL_AMT1   \n",
						"0   1      20000   24      2      2     -1     -1     -2     -2       3913  \\\n",
						"1   2     120000   26     -1      2      0      0      0      2       2682   \n",
						"2   3      90000   34      0      0      0      0      0      0      29239   \n",
						"\n",
						"   ...  default payment next month  SEX_1  SEX_2  EDUCATION_1  EDUCATION_2   \n",
						"0  ...                           1  False   True        False         True  \\\n",
						"1  ...                           1  False   True        False         True   \n",
						"2  ...                           0  False   True        False         True   \n",
						"\n",
						"   EDUCATION_3  EDUCATION_4  MARRIAGE_1  MARRIAGE_2  MARRIAGE_3  \n",
						"0        False        False        True       False       False  \n",
						"1        False        False       False        True       False  \n",
						"2        False        False       False        True       False  \n",
						"\n",
						"[3 rows x 31 columns]\n"
					]
				}
			],
			"source": [
				"# One-hot-encoding categorical columns\n",
				"categorical = [\"SEX\", \"EDUCATION\", \"MARRIAGE\"]\n",
				"one_hot_df = pd.get_dummies(df, columns=categorical)\n",
				"\n",
				"print(one_hot_df.head(3))"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### 4. Z-score normalization on LIMIT_BAL and AGE"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 4,
			"metadata": {
				"scrolled": true
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"   ID  LIMIT_BAL       AGE  PAY_0  PAY_2  PAY_3  PAY_4  PAY_5  PAY_6   \n",
						"0   1  -1.136720 -1.246020      2      2     -1     -1     -2     -2  \\\n",
						"1   2  -0.365981 -1.029047     -1      2      0      0      0      2   \n",
						"2   3  -0.597202 -0.161156      0      0      0      0      0      0   \n",
						"\n",
						"   BILL_AMT1  ...  default payment next month  SEX_1  SEX_2  EDUCATION_1   \n",
						"0       3913  ...                           1  False   True        False  \\\n",
						"1       2682  ...                           1  False   True        False   \n",
						"2      29239  ...                           0  False   True        False   \n",
						"\n",
						"   EDUCATION_2  EDUCATION_3  EDUCATION_4  MARRIAGE_1  MARRIAGE_2  MARRIAGE_3  \n",
						"0         True        False        False        True       False       False  \n",
						"1         True        False        False       False        True       False  \n",
						"2         True        False        False       False        True       False  \n",
						"\n",
						"[3 rows x 31 columns]\n"
					]
				}
			],
			"source": [
				"from sklearn.preprocessing import StandardScaler\n",
				"\n",
				"scaler = StandardScaler()\n",
				"\n",
				"one_hot_df[\"LIMIT_BAL\"] = scaler.fit_transform(one_hot_df[[\"LIMIT_BAL\"]])\n",
				"# TODO maybe min-max norm instead?\n",
				"one_hot_df[\"AGE\"] = scaler.fit_transform(one_hot_df[[\"AGE\"]])\n",
				"\n",
				"print(one_hot_df.head(3))"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### 5. Min-max normalization on PAY_n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 6,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"   ID  LIMIT_BAL       AGE  PAY_0  PAY_2  PAY_3  PAY_4  PAY_5  PAY_6   \n",
						"0   1  -1.136720 -1.246020    0.4    0.4    0.1    0.1    0.0    0.0  \\\n",
						"1   2  -0.365981 -1.029047    0.1    0.4    0.2    0.2    0.2    0.4   \n",
						"2   3  -0.597202 -0.161156    0.2    0.2    0.2    0.2    0.2    0.2   \n",
						"\n",
						"   BILL_AMT1  ...  default payment next month  SEX_1  SEX_2  EDUCATION_1   \n",
						"0       3913  ...                           1  False   True        False  \\\n",
						"1       2682  ...                           1  False   True        False   \n",
						"2      29239  ...                           0  False   True        False   \n",
						"\n",
						"   EDUCATION_2  EDUCATION_3  EDUCATION_4  MARRIAGE_1  MARRIAGE_2  MARRIAGE_3  \n",
						"0         True        False        False        True       False       False  \n",
						"1         True        False        False       False        True       False  \n",
						"2         True        False        False       False        True       False  \n",
						"\n",
						"[3 rows x 31 columns]\n"
					]
				}
			],
			"source": [
				"from sklearn.preprocessing import MinMaxScaler\n",
				"\n",
				"min_max_scaler = MinMaxScaler()\n",
				"\n",
				"# Debug print out\n",
				"# for i in range(7):\n",
				"#     if i == 1: continue\n",
				"#     print(f\"PAY_{i}\")\n",
				"#     print(min(one_hot_df[f\"PAY_{i}\"].unique()), max(one_hot_df[f\"PAY_{i}\"].unique()))\n",
				"\n",
				"PAY_N_columns = [\"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\"]\n",
				"\n",
				"one_hot_df[PAY_N_columns] = min_max_scaler.fit_transform(one_hot_df[PAY_N_columns])\n",
				"\n",
				"print(one_hot_df.head(3))"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Ethics & Privacy"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"We recognize that our project may have real data privacy and ethics issues depending on what variables we decide to put into creating our model. Some variables such as Gender, Marriage status and Age may be very sensitive and private information, we will make sure that datas we use are completely anonymous and will not specify or harm any real entities that the data may subject to. Additionally, we will make sure that our model will be for research purposes only and will not be used to discriminate against any group or any person. Lastly, we can utilize tools such as 'deon' to check for data ethics issue."
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Team Expectations "
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"* *Our team will converse with eachother on our group chat*\n",
				"* *Our team will split work accordingly to fairness and ability*\n",
				"* *Our team will schedule meetings within a week in order to accomdoate changing schedules*\n",
				"* *Individidual members of the team are expected to follow the Project Timeline Proposal as best as possible*\n",
				"* *If issues arise with a submission for previously split work, individuals are expected to ask for help from other teammates*"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Project Timeline Proposal"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"Things have been busy, but this is the updated Timeline and future Timeline Proposal for our group.\n",
				"\n",
				"| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
				"|---|---|---|---|\n",
				"| 5/17  |  7 PM |  Individually assigned sections of proposal (all)  | Finalize Project Proposal Submission | \n",
				"| 5/21  |  5 PM |  Review methods for implementation of feature analysis (all) | Figure out which combinations of features produce better models with error metrics discussed prior by assigning models to each person for analysis | \n",
				"| 5/23  | 5 PM  | Make sure project looks presentable for Peer Review (all)  | Clean up project if there's anything that looks rough, discuss EDA individual assigments   |\n",
				"| 5/27  | 5 PM  | EDA assigned responsibilities (all) | Review/Edit wrangling/EDA; Discuss Analysis Plan  |\n",
				"| 5/30  | 5 PM  | Finalize wrangling/EDA; Begin programming for project (all) | Discuss/edit project code; Assure all models followed what we said they did |\n",
				"| 6/04  | 5 PM  | Each model's analysis (all)| Discuss/edit full project |\n",
				"| 6/07  | 5 PM  | NA | Discuss any responsiblities that may need to be taken care of  |\n",
				"| 6/14  | Before 11:59 PM  | NA | Turn in Final Project  |"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Footnotes\n",
				"<a name=\"Akinnote\"></a>1.[^](#Akin): Akin, J. (1 Nov 2022) How Does Default Impact Your Credit?. *Experian*. https://www.experian.com/blogs/ask-experian/how-does-default-impact-credit/ <br>\n",
				"<a name=\"Ouliarisnote\"></a>2.[^](#Ouliaris): Ouliaris, S. (9 Dec 2021) What are Economic Models?. *International Monetary Fund*. https://www.imf.org/external/pubs/ft/fandd/2011/06/basics.html <br>\n",
				"<a name=\"Neilsennote\"></a>3.[^](#Neilsen,): Neilsen, B. (14 Jan 2022) The Lost Decade: Lessons from Japan's Real Estate Crisis. *Investopedia*. https://www.investopedia.com/articles/economics/08/japan-1990s-credit-crunch-liquidity-trap.asp <br>\n",
				"<a name=\"CostaeSilvanote\"></a>4.[^](#CostaeSilva,): Costa e Silva, E. (5 May 2020) TA logistic regression model for consumer default risk. *National Library of Medicine*. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9041570/ <br>\n",
				"<a name=\"Sayjadahnote\"></a>5.[^](#Sayjadah,): Sayjadah, Y. et al. (28 Oct 2018) Credit Card Default Prediction using Machine Learning Techniques. *IEEE Explore*. https://ieeexplore.ieee.org/document/8776802 <br>\n",
				"<a name=\"Mahmudinote\"></a>6.[^](#Mahmudi,): Mahmudi, H. et al. (15 Oct 2022) Evaluation of Gradient Boosting Algorithms on Balanced Home Credit Default Risk. *IEEE Explore*. https://ieeexplore.ieee.org/document/10041584 <br>\n",
				"<a name=\"Guinote\"></a>7.[^](#Gui): Gui, L. (2019) Application of Machine Learning Algorithms in Predicting Credit Card Default Payment. *UCLA*. https://escholarship.org/uc/item/9zg7157q#main <br>"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": []
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Python 3 (ipykernel)",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.9.16"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 2
}
